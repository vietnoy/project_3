{
    "id": 62902,
    "kind": "PDF",
    "auditfirm_id": 6,
    "impact": "MEDIUM",
    "finders_count": 4,
    "protocol_id": 3419,
    "title": "RaptorCast broadcast strategy lacks sufﬁcient redundancy",
    "content": "## Severity: Medium Risk\n\n## Context\n`udp.rs#L735-L772`\n\n## Summary\nThe RaptorCast design calls for sending an excess of blocks to validators to guarantee sufficient redundancy. The algorithm as implemented produces too few blocks, leading to insufficient redundancy.\n\n## Finding Description\nThe raptorcast broadcast algorithm differs from the one in the documentation as described in \"3: Broadcast Strategy\":\n\nThe design of RaptorCast dictates that redundancy is achieved through producing \\( M' = K \\times r + n \\) chunks, with \\( K \\) being the smallest number of chunks that could fit the message, \\( r \\) being a redundancy factor (hard-coded), and \\( n \\) being the number of validators. The number of chunks sent to each validator is a fraction of \\( M \\) corresponding to the validator's proportion of the total stake.\n\nThe algorithm in `udp.rs`, however, produces \\( M = K \\times r \\) blocks and rounds down the number of chunks to send each validator. Validators with small stakes may receive no blocks and instead have their blocks grouped in with the next validator being in the iteration. The discrepancy is thus greater for \\( K \\) (small message sizes) and large \\( n \\) (large validator sets).\n\nThe redundancy design \"guarantees that each honest validator receives sufficient chunks to decode, even with one-third Byzantine nodes.\" Without it, there is a risk of consensus failure in case the number of Byzantine nodes grows past what \\( r \\) accounts for, the network failure is unusually high, or simply when the message size leads to an unfortunate level of rounding down when calculating the number of chunks to send to each validator.\n\n## Impact Explanation\nWith fewer distinct first-hop rebroadcasters per message, any outage or Byzantine behavior by the (few) assigned recipients disproportionately harms overall symbol fanout. Because only the original recipient rebroadcasts, validators assigned zero chunks also contribute zero second-hop bandwidth for that chunk. There is also higher latency to reach sufficient validators, especially under moderate packet loss.\n\n## Likelihood Explanation\nA validator gets 0 chunks if the total stake as a multiple of their individual stake is larger than the number of packets: \n\\[ \\frac{\\text{total\\_stake}}{\\text{stake}_i} > \\text{num\\_packets} \\] \nIt is virtually guaranteed that some validators will receive less than their allotted amount.\n\n## Proof of Concept\nFor example, as a degenerate case, when `num_packets == 1` the `continue` on line 754 will be hit in every iteration. If it's 2, then it will be hit at every iteration unless some validator has more than half of the total stake, etc. Regardless, this will undercount the packages to be sent out and possibly not send all chunks, if the last `end_idx` is less than `chunk_datas.len()`.\n\n## Recommendation\nWhen calculating the number of packets, make the following addition after line 603 in `udp.rs`:\n\n```rust\nif let BuildTarget::Broadcast(epoch_validators) = &build_target {\n    num_packets = num_packets\n        .checked_mul(epoch_validators.view().len())\n        .expect(\"num_packets doesn’t fit in usize \")\n} else if let BuildTarget::Raptorcast((epoch_validators, full_nodes_view)) = &build_target {\n    let n = epoch_validators.view().len();\n    let extra = n.min((MAX_NUM_PACKETS as usize).saturating_sub(num_packets));\n    num_packets = num_packets + extra;\n}\n```\n\nThis does not exactly match the number of packages described in the design, but overallocates reasonably. Another approach to adding redundancy more robustly without too much extra complexity:\n\n- Calculate \\( M' \\) as above, including \\( n \\).\n- Before running the loop assigning stake, subtract \\( n \\) from \\( M' \\) to get \\( M \\).\n- Create a variable \\( i \\) to track which loop iteration you are in.\n- For every validator:\n  - Assign \\( m \\times \\frac{\\text{running\\_stake}}{\\text{total\\_stake}} + i \\) to `start_idx`.\n  - Increment \\( i \\) by 1.\n  - Assign \\( m \\times \\frac{\\text{running\\_stake}}{\\text{total\\_stake}} + i \\) to `end_idx`.\n  - Proceed as before.\n\nStarting at line 739 in `udp.rs`:\n\n```rust\nlet mut running_stake = 0;\nlet mut chunk_idx = 0_u16;\nlet mut nodes: Vec<_> = epoch_validators.view().iter().collect();\nlet mut m = num_packets - epoch_validators.view().len();\n// Group shuffling so chunks for small proposals aren't always assigned\n// to the same nodes, until researchers come up with something better.\nnodes.shuffle(&mut rand::thread_rng());\nlet mut i = 0;\nfor (node_id, validator) in &nodes {\n    let start_idx: usize = i + (m as u64 * running_stake / total_stake) as usize;\n    i += 1;\n    running_stake += validator.stake.0;\n    let end_idx: usize = i + (m as u64 * running_stake / total_stake) as usize;\n    if let Some(addr) = known_addresses.get(node_id) {\n```\n\nThis results in an identical algorithm to the existing implementation except each validator gets exactly one more chunk than they would have otherwise. The difference between this and the algorithm in the design is only that those validators for which \\( ST \\) divides \\( M \\times S_i \\) (i.e. \\( ST \\mid M \\times S_i \\)), will get one more package than they would otherwise. In realistic scenarios with large \\( ST \\), this will be rare.\n\n## Category Labs\nWe are aware of this issue and are working on a redesign that ensures at least probabilistic Byzantine Fault-Tolerance for RaptorCast, and scales well in the number of validators. That solution might not be ready for mainnet, and we are evaluating more short-term fixes.\n\n## Spearbit\nAcknowledged.",
    "summary": "\nThe RaptorCast design currently does not produce enough blocks, leading to insufficient redundancy. This can cause problems such as network failure or consensus failure. The algorithm in `udp.rs` differs from the one described in the documentation, resulting in fewer blocks being sent to validators. This can particularly affect validators with small stakes, as they may receive no blocks at all. To fix this, a recommendation has been made to adjust the algorithm in `udp.rs` to allocate more blocks to validators. The team is aware of this issue and is working on a solution, but it may not be ready for mainnet.",
    "report_date": "2025-09-19T00:00:00.000Z",
    "contest_prize_txt": "",
    "contest_link": "https://github.com/spearbit/portfolio/blob/master/pdfs/Monad-Spearbit-Security-Review-September-2025.pdf",
    "sponsor_name": null,
    "sponsor_link": "",
    "quality_score": 0,
    "general_score": 0,
    "source_link": "https://github.com/spearbit/portfolio/blob/master/pdfs/Monad-Spearbit-Security-Review-September-2025.pdf",
    "github_link": "",
    "pdf_link": "https://solodit-bucket.s3.amazonaws.com/storage/reports/spearbit/Monad-Spearbit-Security-Review-September-2025.pdf",
    "pdf_page_from": 28,
    "contest_id": "",
    "slug": "raptorcast-broadcast-strategy-lacks-sufficient-redundancy-spearbit-none-monad-pdf",
    "firm_name": "Spearbit",
    "firm_logo_square": "spearbit_square.png",
    "protocol_name": "Monad",
    "bookmarked": false,
    "read": false,
    "issues_issue_finders": [
        {
            "wardens_warden": {
                "handle": "Haxatron"
            }
        },
        {
            "wardens_warden": {
                "handle": "Dtheo"
            }
        },
        {
            "wardens_warden": {
                "handle": "Guido Vranken"
            }
        },
        {
            "wardens_warden": {
                "handle": "Rikard Hjort"
            }
        }
    ],
    "auditfirms_auditfirm": {
        "name": "Spearbit",
        "logo_square": "spearbit_square.png"
    },
    "protocols_protocol": {
        "name": "Monad",
        "protocols_protocolcategoryscore": []
    },
    "issues_issuetagscore": []
}